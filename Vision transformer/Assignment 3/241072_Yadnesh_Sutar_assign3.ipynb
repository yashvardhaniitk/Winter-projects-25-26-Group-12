{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmTMSeaTLwVp"
      },
      "source": [
        "**SUBMISSION INSTRUCTIONS**\n",
        "\n",
        "First make a copy of this colab file and then solve the assignment and upload your final notebook on github.\n",
        "\n",
        "**To write an answer to any question, First Add a TEXT block below the question and type it in only one single block**\n",
        "\n",
        "Before uploading your downloaded notebook, RENAME the file as rollno_name.ipynb\n",
        "\n",
        "Submission Deadline : 19/12/2025 Friday EOD i.e before 11:59 PM\n",
        "\n",
        "The deadline is strict and will not be extended, Late submissions are not allowed\n",
        "\n",
        "Note that you have to upload your solution on the github page of the project Vision Transformer and **under Assignment 3**\n",
        "\n",
        "And remember to keep title of your pull request to be ViT_name_rollno_assgn3\n",
        "\n",
        "Github Submission repo -\n",
        "https://github.com/electricalengineersiitk/Winter-projects-25-26/tree/main/Vision%20transformer/Assignment%203"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TjKxTBEz8i7"
      },
      "source": [
        "**THEORETICAL ASSIGNMENT**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnVa1_8AKqtv"
      },
      "source": [
        "This assignment consists of theoretical questions. Objective answers will not be accepted. You need to write detailed answers (200-300 words) for the questions that require more in-depth explanations (you should be able to identify these after reading the question and finding about them). Questions that need less explanation can be answered in 20-100 words. Please ensure the answers are well-written and thorough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERhYz9qV_YXI"
      },
      "source": [
        "1. Assume that the inputs **X**\n",
        " to some scalar function **f**\n",
        " are **n x m**\n",
        " matrices. What is the dimensionality of the gradient of **f**\n",
        " with respect to **X**?\n",
        "**Give reasons to justify your answer.**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANSWER:** The dimentionality of the gradient of **f** wrt **X** is **\"n x m\"**\n",
        "\n",
        "**REASONING:** The gradient of f wrt X is defined as summation/collection of partial derivatives of f wrt all elements of X, in this case there are maximum (n x m) elememts/terms. therefore the gradient has one term for each term in the matrix X. Thus the total terms in the gradient is n x m."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vnyYHlq2g4B"
      },
      "source": [
        "2. Define the evaluation metrics Accuracy, Precision, Recall, and F1 score.\n",
        "\n",
        "   Consider the following diagram:\n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score.\n",
        "   |                | Actual: Cancer | Actual: No Cancer |\n",
        "   |----------------|---------------|------------------|\n",
        "   | Predicted: Cancer | 80 | 80 |\n",
        "   | Predicted: No Cancer | 20 | 820 |\n",
        "    \n",
        "   \n",
        "   Find the values of Accuracy, Precision, Recall, and F1 score using the above data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Defintations:**\n",
        "\n",
        "**Accuracy:** Accuracy measures the proportion of total correct predictions made by a model, considering both true positives and true negatives.\n",
        "\n",
        "**Precision:** Precision measures how many predicted positive cases are actually correct, indicating the model’s reliability when identifying positive outcomes.\n",
        "\n",
        "**Recall:** Recall measures how many actual positive cases the model correctly identifies, showing its effectiveness at detecting true positive instances.\n",
        "\n",
        "**F1 score:** F1 score combines precision and recall using a harmonic mean, providing a balanced measure of a model’s performance on positive predictions.\n",
        "\n",
        "\n",
        "**CALCULATIONS:**\n",
        "\n",
        "**Accuracy**= (correct predictions)/(Total Predictions) = (80+820)/1000=0.9\n",
        "\n",
        "**Precision**= 80/(80+80) = 0.5\n",
        "\n",
        "**Recall**= 80/(80+20) = 0.8\n",
        "\n",
        "**F1 score** = 2*precision*recall/(precision+recall) = 2*0.5*0.8/(0.5+0.8) = 0.615\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1dJMD0vJKZg"
      },
      "source": [
        "3. What is a confusion matrix ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A confusion matrix is a performance evaluation table used in classification tasks that compares predicted labels with actual labels. \n",
        "It contains four values: True Positives, False Positives, True Negatives, and False Negatives.\n",
        " It helps analyze model accuracy, types of errors, class imbalance issues, and provides the basis for metrics like precision, recall, and F1 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOqF8Dzw0DdZ"
      },
      "source": [
        "4. What is overfitting and underfitting ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overfitting occurs when a model learns the training data too closely, including noise and outliers, resulting in high training accuracy but poor performance on new data. It indicates poor generalization.\n",
        "\n",
        "Underfitting occurs when a model is too simple or insufficiently trained, failing to learn important patterns in data, leading to high error on both training and test sets. It indicates the model lacks complexity or sufficient learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4KfKc0z0ulm"
      },
      "source": [
        "5. Explain vanishing and exploding gradients, give reasons why they occur and how they can be prevented.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vanishing gradients occur in deep networks when gradients become extremely small during backpropagation,\n",
        " causing earlier layers to learn very slowly or stop learning. Exploding gradients occur when gradients grow excessively large, \n",
        " causing unstable weight updates and divergence. These problems arise due to repeated multiplication in deep networks,\n",
        " inappropriate activation functions, or poor weight initialization. Prevention methods include ReLU activations, \n",
        " gradient clipping, batch normalization, and proper initialization (Xavier/He). LSTMs/GRUs also help mitigate vanishing gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yaOipvU1JoD"
      },
      "source": [
        "6. What is regularization ? Explain L1 and L2 regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Regularization is a technique used to reduce overfitting by adding a penalty to the loss function that discourages large weights.\n",
        "\n",
        "L1 regularization adds the absolute value of weights to the loss, promoting sparsity by forcing some weights to become zero.\n",
        "\n",
        "L2 regularization adds the squared value of weights to the loss, discouraging large weights and distributing them more evenly. Both help improve generalization and stability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjeRGpCE7QJ_"
      },
      "source": [
        "7. What is Dropout Layer and To which part of a neural network is dropout generally applied? how does it prevent overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A Dropout layer randomly disables a percentage of neurons during training, typically in hidden layers of neural networks. By dropping units and connections, the model is forced to learn multiple independent representations instead of relying on particular neurons. This prevents co-adaptation, reduces overfitting, and improves generalization on unseen data. During testing, dropout is disabled and full network capacity is used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLZFLyv4-A67"
      },
      "source": [
        "8. A hidden layer of a neural network has the following activations:\n",
        "\n",
        "   \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "   Dropout is applied **during training** with dropout probability \\(p = 0.5\\).\n",
        "\n",
        "   A randomly generated dropout mask is:\n",
        "\n",
        "   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "   Here,  \n",
        "   - `1` → neuron is **kept**  \n",
        "   - `0` → neuron is **dropped**\n",
        "\n",
        "   ---\n",
        "\n",
        "   **Question:**  \n",
        "   What will be the output activations of this layer **after applying dropout**?\n",
        "   *(Show the final activation vector)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "  Activations: \\[\n",
        "   a = [2.0, 5.0, 7.12, 4.5, 6.0]\n",
        "   \\]\n",
        "\n",
        "Mask:   \\[\n",
        "   m = [1, 0, 0, 1, 1]\n",
        "   \\]\n",
        "\n",
        "Applying mask ie multiplying element wise:\n",
        "\n",
        "Final Activation: [2.0, 0.0, 0.0, 4.5, 6.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La3K0HmUGj8t"
      },
      "source": [
        "9. Explain the difference between Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Gradient Descent (GD)**: Uses the entire training dataset to compute gradients for each update. It is stable and accurate but computationally expensive and slow for large datasets.\n",
        "\n",
        "**Stochastic Gradient Descent (SGD)**: Updates parameters using a single randomly selected sample per iteration. It is fast and efficient but produces noisy updates, causing fluctuations in the loss function.\n",
        "\n",
        "**Mini-Batch Gradient Descent**: Uses small subsets (batches) of data per update, balancing speed and stability. It is computationally efficient and widely used in deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FngV9WtV7JxI"
      },
      "source": [
        "10. What are optimizers ? Explain Adam, RMS prop and Momentum in detail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Optimizers** are algorithms that adjust neural network parameters to minimize loss during training by deciding the direction and size of weight updates.\n",
        "\n",
        "\n",
        "**Momentum** accelerates gradient descent by adding a fraction of previous gradients, helping overcome local minima and smoothing oscillations.\n",
        "\n",
        "\n",
        "**RMSProp** adapts the learning rate by storing moving averages of squared gradients, preventing large updates and improving training on non-stationary problems.\n",
        "\n",
        "**Adam** combines Momentum and RMSProp, using first (mean) and second (variance) moment estimates, offering fast convergence and adaptive learning, making it highly effective in deep learning."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
