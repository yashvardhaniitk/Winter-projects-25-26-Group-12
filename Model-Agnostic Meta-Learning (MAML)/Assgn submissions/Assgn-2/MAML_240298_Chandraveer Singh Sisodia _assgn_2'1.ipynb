{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5y_HbJiPKhA"
      },
      "source": [
        "# Linear Regression\n",
        "## Question 1\n",
        "Make a class called LinearRegression which provides two functions : fit and predict. Try to implement it from scratch. If stuck, refer to the examples folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzoG2XilPLFr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegression:\n",
        "    def __init__(self):\n",
        "        self.slope = None\n",
        "        self.intercept = None                                      #we find the value of slope for which distance between y values of predicted and actual data is minimum and got\n",
        "                                                                   #desired value of slope whcih is used in the code\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        #mean calculation\n",
        "        x_mean = np.mean(X)\n",
        "        y_mean = np.mean(y)\n",
        "\n",
        "        #putting values of mean in formula e got from differentiating y difference  values\n",
        "        numerator = np.sum((X - x_mean) * (y - y_mean))\n",
        "        denominator = np.sum((X - x_mean)**2)\n",
        "\n",
        "        # Calculate coefficients\n",
        "        self.slope = numerator / denominator\n",
        "        self.intercept = y_mean - (self.slope * x_mean)\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        return self.intercept + self.slope * X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsqoxNag7D3-"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Use the dataset https://www.kaggle.com/datasets/quantbruce/real-estate-price-prediction (*).\n",
        "1. Read it using pandas.\n",
        "2. Check for **null values**.\n",
        "3. For each of the columns (except the first and last), plot the column values in the X-axis against the last column of prices in the Y-axis.\n",
        "4. Remove the unwanted columns.\n",
        "5. Split the dataset into train and test data. Test data size = 25% of total dataset.\n",
        "6. **Normalize** the X_train and X_test using MinMaxScaler from sklearn.preprocessing.\n",
        "7. Fit the training data into the model created in question 1 and predict the testing data.\n",
        "8. Use **mean square error and R<sup>2</sup>** from sklearn.metrics as evaluation criterias.\n",
        "9. Fit the training data into the models of the same name provided by sklearn.linear_model and evaluate the predictions using MSE and R<sup>2</sup>.\n",
        "10. Tune the hyperparameters of your models (learning rate, epochs) to achieve losses close to that of the sklearn models.\n",
        "\n",
        "Note : (*) To solve this question, you may proceed in any of the following ways :\n",
        "1. Prepare the notebook in Kaggle, download it and submit it separately with the other questions.\n",
        "2. Download the dataset from kaggle. Upload it to the session storage in Colab.\n",
        "3. Use Colab data directly in Colab. [Refer here](https://www.kaggle.com/general/74235). For this, you need to create kaggle API token. Before submitting, hide or remove the API token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lupaMcr63QF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e4157c-82c8-43c0-8248-7d5ee82ff1f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null Values:\n",
            " No                                        0\n",
            "X1 transaction date                       0\n",
            "X2 house age                              0\n",
            "X3 distance to the nearest MRT station    0\n",
            "X4 number of convenience stores           0\n",
            "X5 latitude                               0\n",
            "X6 longitude                              0\n",
            "Y house price of unit area                0\n",
            "dtype: int64\n",
            "\n",
            "Tuning Custom Model:\n",
            "LR: 0.1, Epochs: 5000 -> MSE: 66.0170, R2: 0.5838\n",
            "LR: 0.1, Epochs: 10000 -> MSE: 66.6232, R2: 0.5800\n",
            "LR: 0.05, Epochs: 5000 -> MSE: 65.2572, R2: 0.5886\n",
            "LR: 0.05, Epochs: 10000 -> MSE: 66.0168, R2: 0.5838\n",
            "LR: 0.01, Epochs: 5000 -> MSE: 65.7928, R2: 0.5852\n",
            "LR: 0.01, Epochs: 10000 -> MSE: 65.0616, R2: 0.5898\n",
            "\n",
            "Final Results:\n",
            "Sklearn Model: MSE=66.7486, R2=0.5792\n",
            "Best Custom Model ({'lr': 0.01, 'epochs': 10000}): MSE=65.0616, R2=0.5898\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression as SklearnLinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "df = pd.read_csv('Real estate.csv')\n",
        "null = df.isnull().sum()\n",
        "print(\"Null Values:\\n\", null)\n",
        "features = df.columns[1:-1]\n",
        "target_col = df.columns[-1]\n",
        "\n",
        "for i, col in enumerate(features):\n",
        "    plt.scatter(df[col], df[target_col], alpha=0.5)\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel(target_col)\n",
        "    plt.title(f\"{col} vs Price\")\n",
        "    plt.savefig(f\"plot_{i}.png\")\n",
        "    plt.close()\n",
        "\n",
        "X_raw = df.drop(columns=['No', 'X1 transaction date', target_col])\n",
        "y_raw = df[target_col].values\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X_raw, y_raw, test_size=0.25, random_state=42)\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train_raw)\n",
        "X_test = scaler.transform(X_test_raw)\n",
        "class CustomLinearRegression:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            y_predicted = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            # Gradients\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # Update parameters\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.dot(X, self.weights) + self.bias\n",
        "\n",
        "def evaluate(model, X, y_true):\n",
        "\n",
        "  preds = model.predict(X)\n",
        "  mse = mean_squared_error(y_true, preds)\n",
        "  r2 = r2_score(y_true, preds)\n",
        "  return mse, r2\n",
        "sk_model = SklearnLinearRegression()\n",
        "sk_model.fit(X_train, y_train)\n",
        "sk_mse, sk_r2 = evaluate(sk_model, X_test, y_test)\n",
        "\n",
        "best_custom_mse = float('inf')\n",
        "best_params = {}\n",
        "lrs = [0.1, 0.05, 0.01]\n",
        "epochs_list = [5000, 10000]\n",
        "\n",
        "print(\"\\nTuning Custom Model:\")\n",
        "for lr in lrs:\n",
        "    for ep in epochs_list:\n",
        "        model = CustomLinearRegression(learning_rate=lr, epochs=ep)\n",
        "        model.fit(X_train, y_train)\n",
        "        mse, r2 = evaluate(model, X_test, y_test)\n",
        "        print(f\"LR: {lr}, Epochs: {ep} -> MSE: {mse:.4f}, R2: {r2:.4f}\")\n",
        "        if mse < best_custom_mse:\n",
        "            best_custom_mse = mse\n",
        "            best_custom_r2 = r2\n",
        "            best_params = {'lr': lr, 'epochs': ep}\n",
        "\n",
        "# Final output for report\n",
        "print(\"\\nFinal Results:\")\n",
        "print(f\"Sklearn Model: MSE={sk_mse:.4f}, R2={sk_r2:.4f}\")\n",
        "print(f\"Best Custom Model ({best_params}): MSE={best_custom_mse:.4f}, R2={best_custom_r2:.4f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQ7lQpy-SYCq"
      },
      "source": [
        "# Logistic Regression\n",
        "## Question 3\n",
        "\n",
        "The breast cancer dataset is a binary classification dataset commonly used in machine learning tasks. It is available in scikit-learn (sklearn) as part of its datasets module.\n",
        "Here is an explanation of the breast cancer dataset's components:\n",
        "\n",
        "* Features (X):\n",
        "\n",
        " * The breast cancer dataset consists of 30 numeric features representing different characteristics of the FNA images. These features include mean, standard error, and worst (largest) values of various attributes such as radius, texture, smoothness, compactness, concavity, symmetry, fractal dimension, etc.\n",
        "\n",
        "* Target (y):\n",
        "\n",
        " * The breast cancer dataset is a binary classification problem, and the target variable (y) represents the diagnosis of the breast mass. It contains two classes:\n",
        "    * 0: Represents a malignant (cancerous) tumor.\n",
        "    * 1: Represents a benign (non-cancerous) tumor.\n",
        "\n",
        "Complete the code given below in place of the \"...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auipk-zBpmO-"
      },
      "source": [
        "1. Load the dataset from sklearn.datasets\n",
        "2. Separate out the X and Y columns.\n",
        "3. Normalize the X data using MinMaxScaler or StandardScaler.\n",
        "4. Create a train-test-split. Take any suitable test size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OyGNHNjFh13"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression as SklearnLogistic\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM-SsSxpqF2o"
      },
      "source": [
        "5. Write code for the sigmoid function and Logistic regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o81LA5MZFoTW"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    # This function \"squashes\" the linear input into a range of [0, 1]\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    # Useful for understanding the slope of the curve at any point\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate, epochs):\n",
        "        # Initialise the hyperparameters of the model\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "        # Initialise weights as zeros and bias as 0\n",
        "        self.weights = np.zeros((n_features, 1))\n",
        "        self.bias = 0\n",
        "\n",
        "        # Implement the GD algorithm\n",
        "        for _ in range(self.epochs):\n",
        "            # 1. Linear combination of features\n",
        "            z = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            # 2. Map to probability using sigmoid\n",
        "            y_pred = sigmoid(z)\n",
        "\n",
        "            # 3. Calculate gradients (Partial derivatives of Loss)\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # 4. Update the knowledge of the model\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Write the predict function\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        probabilities = sigmoid(z)\n",
        "\n",
        "        # Convert probabilities to discrete classes: 0 or 1\n",
        "        # We typically use a threshold of 0.5\n",
        "        y_pred = np.array([1 if p >= 0.5 else 0 for p in probabilities])\n",
        "\n",
        "        return y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo9LNRMzq4K-"
      },
      "source": [
        "6. Fit your model on the dataset and make predictions.\n",
        "7. Compare your model with the Sklearn Logistic Regression model. Try out all the different penalties.\n",
        "8. Print accuracy_score in each case using sklearn.metrics ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyGsTTOqFphf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b468d90f-1416-4465-e062-7ba5485284a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 Predictions:    [1 0 0 1 1 0 0 0 1 1]\n",
            "First 10 Actual Labels:  [1 0 0 1 1 0 0 0 1 1]\n",
            "Sklearn Accuracy (penalty=l2): 0.9737\n",
            "Sklearn Accuracy (penalty=None): 0.9386\n",
            "\n",
            "--- Custom Model Report ---\n",
            "Accuracy: 0.9912\n",
            "Confusion Matrix:\n",
            " [[42  1]\n",
            " [ 0 71]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99        43\n",
            "           1       0.99      1.00      0.99        71\n",
            "\n",
            "    accuracy                           0.99       114\n",
            "   macro avg       0.99      0.99      0.99       114\n",
            "weighted avg       0.99      0.99      0.99       114\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression as SklearnLogistic\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "def sigmoid(z):\n",
        "    # This function \"squashes\" the linear input into a range of [0, 1]\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    # Useful for understanding the slope of the curve at any point\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate, epochs):\n",
        "        # Initialise the hyperparameters of the model\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        y = y.reshape(-1, 1)\n",
        "\n",
        "        # Initialise weights as zeros and bias as 0\n",
        "        self.weights = np.zeros((n_features, 1))\n",
        "        self.bias = 0\n",
        "\n",
        "        # Implement the GD algorithm\n",
        "        for _ in range(self.epochs):\n",
        "            # 1. Linear combination of features\n",
        "            z = np.dot(X, self.weights) + self.bias\n",
        "\n",
        "            # 2. Map to probability using sigmoid\n",
        "            y_pred = sigmoid(z)\n",
        "\n",
        "            # 3. Calculate gradients (Partial derivatives of Loss)\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
        "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # 4. Update the knowledge of the model\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Write the predict function\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        probabilities = sigmoid(z)\n",
        "\n",
        "        # Convert probabilities to discrete classes: 0 or 1\n",
        "        # We typically use a threshold of 0.5\n",
        "        y_pred = np.array([1 if p >= 0.5 else 0 for p in probabilities])\n",
        "\n",
        "        return y_pred\n",
        "\n",
        "my_model = LogisticRegression(learning_rate=0.1, epochs=2000)\n",
        "my_model.fit(X_train, y_train)\n",
        "my_preds = my_model.predict(X_test)\n",
        "print(\"First 10 Predictions:   \", my_preds[:10])\n",
        "print(\"First 10 Actual Labels: \", y_test[:10])\n",
        "\n",
        "penalties = ['l2', None] # Note: 'l1' often requires a different solver like 'liblinear'\n",
        "for p in penalties:\n",
        "    sk_model = SklearnLogistic(penalty=p, solver='lbfgs', max_iter=5000)\n",
        "    sk_model.fit(X_train, y_train)\n",
        "    sk_preds = sk_model.predict(X_test)\n",
        "    print(f\"Sklearn Accuracy (penalty={p}): {accuracy_score(y_test, sk_preds):.4f}\")\n",
        "\n",
        "print(\"\\n--- Custom Model Report ---\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, my_preds):.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, my_preds))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, my_preds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGBkzAO5red4"
      },
      "source": [
        "9. For the best model in each case (yours and scikit-learn), print the classification_report using sklearn.metrics .\n",
        "10. For the best model in each case (yours and scikit-learn), print the confusion_matrix using sklearn.metrics ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le-HfABsvnyF"
      },
      "outputs": [],
      "source": [
        "#it is done in above code space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OQ2tSp0MO6n"
      },
      "source": [
        "# KNN\n",
        "## Question 4\n",
        "\n",
        "How accurately can a K-Nearest Neighbors (KNN) model classify different types of glass based on a glass classification dataset consisting of 214 samples and 7 classes? Use the kaggle dataset \"https://www.kaggle.com/datasets/uciml/glass\".\n",
        "\n",
        "Context: This is a Glass Identification Data Set from UCI. It contains 10 attributes including id. The response is glass type(discrete 7 values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMGxbtX-zfsI"
      },
      "source": [
        "1. Load the data as you did in the 2nd question.\n",
        "2. Extract the X and Y columns.\n",
        "3. Split it into training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p0SfLB7pO7_z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3902f286-7afc-4b4e-f7ad-cc154dd7d172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- PREDICTIONS COMPARISON (First 10) ---\n",
            "Actual Labels:  [1 7 1 7 2 2 1 2 2 2]\n",
            "Custom Preds:   [1 7 1 7 2 2 1 2 2 2]\n",
            "Sklearn Preds:  [1 7 1 7 2 2 1 2 2 2]\n",
            "Custom Model Accuracy (k=3): 0.7674\n",
            "Sklearn Model Accuracy (k=3): 0.7674\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "df = pd.read_csv('glass.csv')\n",
        "X = df.iloc[:, :-1].values\n",
        "y = df.iloc[:, -1].values\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "class KNN:\n",
        "    def __init__(self, k=3):\n",
        "        self.k = k\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # KNN is a \"lazy\" learner; it just stores the training data\n",
        "        self.X_train = np.array(X)\n",
        "        self.y_train = np.array(y)\n",
        "\n",
        "    def _euclidean_distance(self, x1, x2):\n",
        "        return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        predictions = [self._predict(x) for x in X_test]\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def _predict(self, x):\n",
        "        # Calculate distances to all training points\n",
        "        distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]\n",
        "        # Sort and get the indices of the K nearest neighbors\n",
        "        k_indices = np.argsort(distances)[:self.k]\n",
        "        # Get the labels of those neighbors\n",
        "        k_labels = [self.y_train[i] for i in k_indices]\n",
        "        # Majority vote\n",
        "        return max(set(k_labels), key=k_labels.count)\n",
        "knn_custom = KNN(k=3)\n",
        "knn_custom.fit(X_train, y_train)\n",
        "custom_preds = knn_custom.predict(X_test)\n",
        "knn_sklearn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_sklearn.fit(X_train, y_train)\n",
        "sklearn_preds = knn_sklearn.predict(X_test)\n",
        "print(\"--- PREDICTIONS COMPARISON (First 10) ---\")\n",
        "print(f\"Actual Labels:  {y_test[:10]}\")\n",
        "print(f\"Custom Preds:   {custom_preds[:10]}\")\n",
        "print(f\"Sklearn Preds:  {sklearn_preds[:10]}\")\n",
        "print(f\"Custom Model Accuracy (k=3): {accuracy_score(y_test, custom_preds):.4f}\")\n",
        "print(f\"Sklearn Model Accuracy (k=3): {accuracy_score(y_test, sklearn_preds):.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtyZJXh9zoh5"
      },
      "source": [
        "4. Define Euclidean distance.\n",
        "5. Build the KNN model.\n",
        "6. Fit the model on the training data. (Note : you may require to change the type of the data from pandas dataframe to numpy arrays. To do that, just do this X=np.array(X) and so on...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJkhLORLzn6r"
      },
      "outputs": [],
      "source": [
        "#done in first coding block of this question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9rxZpPB0pVS"
      },
      "source": [
        "7. Make predictions. Find their accuracy using accuracy_score. Try different k values. k=3 worked well in our case.\n",
        "8. Compare with the sklearn model (from sklearn.neighbors import KNeighborsClassifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ernfjaZJ0pAh"
      },
      "outputs": [],
      "source": [
        "#done in first coding block of this question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2tZQg4L09wn"
      },
      "outputs": [],
      "source": [
        "#done in first coding block of this question"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}