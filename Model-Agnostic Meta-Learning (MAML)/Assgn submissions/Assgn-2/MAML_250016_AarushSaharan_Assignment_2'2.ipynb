{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR6x2V8BgdDM"
      },
      "source": [
        "# Resources on Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "990453b1"
      },
      "source": [
        "from GFG :\n",
        "\n",
        "[Stochastic Gradient Descent:](https://www.geeksforgeeks.org/machine-learning/ml-stochastic-gradient-descent-sgd/)\n",
        "\n",
        "And a Medium article :\n",
        "\n",
        "[Stochastic Gradient Descent:](https://mohitmishra786687.medium.com/stochastic-gradient-descent-a-basic-explanation-cbddc63f08e0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOFl0t--Zhxg"
      },
      "source": [
        "# Question 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a84c921a"
      },
      "source": [
        "How does the learning rate affect the convergence of Stochastic Gradient Descent, and what are some common strategies for choosing or adapting the learning rate during training?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ANSWER:\n",
        "\n",
        "The learning rate controls how big each update step is in Stochastic Gradient Descent. If it is too large, the algorithm may overshoot the minimum, causing the loss to oscillate or even diverge. If it is too small, convergence becomes very slow and training may stall near flat regions. A well-chosen learning rate balances speed and stability, allowing the loss to decrease smoothly.\n",
        "\n",
        "Common strategies include using a fixed learning rate chosen by trial and error, applying learning rate decay (such as step, exponential, or time-based decay) to reduce the step size during training, and using adaptive methods like Adam or RMSProp that automatically adjust the learning rate for each parameter. In practice, learning rate schedules or adaptive optimizers are often preferred for reliable convergence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cG-zPR3Xgfx"
      },
      "source": [
        "\n",
        "#  Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SshnjcyDW5mt"
      },
      "source": [
        "`Gradient Descent vs Stochastic Gradient Descent`\n",
        "\n",
        "Using the same preprocessed dataset from Question 2 from assignment-2'1, do the following:\n",
        "\n",
        "a) Train a Linear Regression model using Batch Gradient Descent (GD)\n",
        "\n",
        "b) Train a Linear Regression model using Stochastic Gradient Descent (SGD)\n",
        "\n",
        "c) Choose suitable values for learning rate and number of epochs.\n",
        "\n",
        "d) Predict house prices for the test dataset using both models.\n",
        "\n",
        "e) Evaluate both models using:\n",
        "Mean Squared Error (MSE) / R² Score\n",
        "\n",
        "f) Print the evaluation results of GD and SGD in a clear comparison format.\n",
        "\n",
        "g) Change the learning rate and epochs of the SGD model and observe how the performance changes.\n",
        "\n",
        "h) Explain why does the SGD path behave so erratically compared to the GD path, and despite this \"noise,\" why might SGD be preferred for very large datasets?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"Real estate.csv\")\n",
        "\n",
        "\n",
        "X = df.drop(\"Y house price of unit area\", axis=1)\n",
        "y = df[\"Y house price of unit area\"]\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "class LinearRegressionGD:\n",
        "    def __init__(self, lr=0.01, epochs=1000):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n = X.shape\n",
        "        self.w = np.zeros(n)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            y_pred = X @ self.w + self.b\n",
        "            dw = (1/m) * X.T @ (y_pred - y)\n",
        "            db = (1/m) * np.sum(y_pred - y)\n",
        "\n",
        "            self.w -= self.lr * dw\n",
        "            self.b -= self.lr * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X @ self.w + self.b\n",
        "\n",
        "gd = LinearRegressionGD(lr=0.01, epochs=1000)\n",
        "gd.fit(X_train, y_train)\n",
        "\n",
        "y_pred_gd = gd.predict(X_test)\n",
        "\n",
        "\n",
        "class LinearRegressionSGD:\n",
        "    def __init__(self, lr=0.01, epochs=50):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        m, n = X.shape\n",
        "        self.w = np.zeros(n)\n",
        "        self.b = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            for i in range(m):\n",
        "                y_pred = X[i] @ self.w + self.b\n",
        "                error = y_pred - y.iloc[i]\n",
        "\n",
        "                self.w -= self.lr * error * X[i]\n",
        "                self.b -= self.lr * error\n",
        "\n",
        "    def predict(self, X):\n",
        "        return X @ self.w + self.b\n",
        "\n",
        "sgd = LinearRegressionSGD(lr=0.01, epochs=50)\n",
        "sgd.fit(X_train, y_train)\n",
        "\n",
        "y_pred_sgd = sgd.predict(X_test)\n",
        "\n",
        "print(\"Model Comparison:\\n\")\n",
        "print(f\"Batch GD  -> MSE: {mse_gd:.3f}, R²: {r2_gd:.3f}\")\n",
        "print(f\"SGD       -> MSE: {mse_sgd:.3f}, R²: {r2_sgd:.3f}\")\n",
        "\n",
        "sgd_fast = LinearRegressionSGD(lr=0.05, epochs=100)\n",
        "sgd_fast.fit(X_train, y_train)\n",
        "\n",
        "y_pred_fast = sgd_fast.predict(X_test)\n",
        "\n",
        "print(\"Modified SGD:\")\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred_fast))\n",
        "print(\"R²:\", r2_score(y_test, y_pred_fast))\n",
        "\n",
        "\"\"\"\n",
        "Why is the SGD path more erratic than GD?\n",
        "Batch Gradient Descent uses the average gradient over the entire dataset, so updates are smooth and stable. \n",
        "Stochastic Gradient Descent updates parameters using one data point at a time, which introduces randomness and causes noisy, zig-zag updates.\n",
        "\n",
        "Why is SGD still preferred for very large datasets?\n",
        "Despite the noise, SGD is computationally much faster per update and scales well to large datasets. \n",
        "It often reaches a good solution faster than GD and can escape shallow local minima due to its randomness.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjQrDrC-Qgbp"
      },
      "source": [
        "# Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8w1SrPMEOnO"
      },
      "source": [
        "## Decision Trees\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r-F4bUvQ43J"
      },
      "source": [
        "### 3.1 Theoretical and Numerical Questions\n",
        "\n",
        "a) Is a **Decision Tree** a supervised or unsupervised learning algorithm?  \n",
        "Give a brief explanation.\n",
        "\n",
        "b) What is **entropy** in the context of decision trees?\n",
        "\n",
        "c) What does **reduction in entropy** signify when a node is split in a decision tree?\n",
        "\n",
        "d) You are given a dataset consisting of **10 data points**, each having:\n",
        "- A class label (+ or −)\n",
        "- A 2D feature vector $(x, y)$\n",
        "\n",
        "All data points are initially present at the **root node** of a decision tree.\n",
        "\n",
        "A **decision stump** (depth = 1 decision tree) is to be learned at the root using the **entropy reduction principle**.\n",
        "\n",
        "**Allowed split questions:**\n",
        "\n",
        "\n",
        "- ($x \\le -2$?)\n",
        "- ($x \\le 2$?)\n",
        "- ($y \\le 2$?)\n",
        "\n",
        "**Assumptions:**\n",
        "- All logarithms are **base 2**\n",
        "\n",
        "\n",
        "- $\\log_2 3 = 1.58$\n",
        "- $\\log_2 5 = 2.32$\n",
        "\n",
        "- Give answers **correct to at least 2 decimal places**\n",
        "\n",
        "|S.No. | Class | (x, y) |\n",
        "|----|-------|--------|\n",
        "| 1  | − | (−3, 0) |\n",
        "| 2  | + | (3, 3) |\n",
        "| 3  | + | (1, 1) |\n",
        "| 4  | + | (1, −1) |\n",
        "| 5  | + | (−1, 1) |\n",
        "| 6  | + | (−1, −1) |\n",
        "| 7  | − | (1, 5) |\n",
        "| 8  | − | (1, 3) |\n",
        "| 9  | − | (−1, 5) |\n",
        "| 10 | − | (−1, 3) |\n",
        "\n",
        "\n",
        "Answer the following:\n",
        "1. Compute the **entropy of the root node**\n",
        "2. Compute the **entropy of the two child nodes** for each allowed split\n",
        "3. Compute the **reduction in entropy** for each split\n",
        "4. Identify **which split should be chosen** based on maximum entropy reduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3.1 Theoretical and Numerical Questions\n",
        "\n",
        "---\n",
        "\n",
        "## (a) Is a Decision Tree supervised or unsupervised?\n",
        "\n",
        "A **Decision Tree is a supervised learning algorithm** because it is trained using labeled data and learns decision rules to predict the target class or value.\n",
        "\n",
        "---\n",
        "\n",
        "## (b) What is entropy in decision trees?\n",
        "\n",
        "**Entropy** is a measure of impurity or uncertainty in a dataset.  \n",
        "It is defined as:\n",
        "\n",
        "$$\n",
        "H(S) = -\\sum_i p_i \\log_2 p_i\n",
        "$$\n",
        "\n",
        "where \\( p_i \\) is the proportion of samples belonging to class \\( i \\).\n",
        "\n",
        "---\n",
        "\n",
        "## (c) What does reduction in entropy signify?\n",
        "\n",
        "**Reduction in entropy (Information Gain)** indicates how much uncertainty is reduced after a split.  \n",
        "A higher reduction means the split separates the classes more effectively.\n",
        "\n",
        "---\n",
        "\n",
        "## (d) Numerical Problem\n",
        "\n",
        "### Dataset summary\n",
        "- Total data points = 10  \n",
        "- Positive (+) = 5  \n",
        "- Negative (−) = 5  \n",
        "\n",
        "---\n",
        "\n",
        "## 1. Entropy of the root node\n",
        "\n",
        "$$\n",
        "H_{\\text{root}}\n",
        "= -\\left(\\frac{5}{10}\\log_2\\frac{5}{10}\n",
        "+ \\frac{5}{10}\\log_2\\frac{5}{10}\\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "H_{\\text{root}} = -(0.5 \\cdot -1 + 0.5 \\cdot -1) = \\mathbf{1.00}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Entropy of child nodes for each split\n",
        "\n",
        "---\n",
        "\n",
        "### Split 1: \\( x \\le -2 \\)\n",
        "\n",
        "**Left child (\\( x \\le -2 \\))**  \n",
        "- Points = 1  \n",
        "- (+, −) = (0, 1)\n",
        "\n",
        "$$\n",
        "H_L = 0\n",
        "$$\n",
        "\n",
        "**Right child (\\( x > -2 \\))**  \n",
        "- Points = 9  \n",
        "- (+, −) = (5, 4)\n",
        "\n",
        "$$\n",
        "H_R =\n",
        "-\\left(\\frac{5}{9}\\log_2\\frac{5}{9}\n",
        "+ \\frac{4}{9}\\log_2\\frac{4}{9}\\right)\n",
        "\\approx 0.99\n",
        "$$\n",
        "\n",
        "**Weighted entropy**\n",
        "\n",
        "$$\n",
        "H = \\frac{1}{10}(0) + \\frac{9}{10}(0.99) = 0.89\n",
        "$$\n",
        "\n",
        "**Entropy reduction**\n",
        "\n",
        "$$\n",
        "\\Delta H = 1.00 - 0.89 = \\mathbf{0.11}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Split 2: \\( x \\le 2 \\)\n",
        "\n",
        "**Left child (\\( x \\le 2 \\))**  \n",
        "- Points = 8  \n",
        "- (+, −) = (4, 4)\n",
        "\n",
        "$$\n",
        "H_L = 1.00\n",
        "$$\n",
        "\n",
        "**Right child (\\( x > 2 \\))**  \n",
        "- Points = 2  \n",
        "- (+, −) = (1, 1)\n",
        "\n",
        "$$\n",
        "H_R = 1.00\n",
        "$$\n",
        "\n",
        "**Weighted entropy**\n",
        "\n",
        "$$\n",
        "H = \\frac{8}{10}(1.00) + \\frac{2}{10}(1.00) = 1.00\n",
        "$$\n",
        "\n",
        "**Entropy reduction**\n",
        "\n",
        "$$\n",
        "\\Delta H = 1.00 - 1.00 = \\mathbf{0.00}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Split 3: \\( y \\le 2 \\)\n",
        "\n",
        "**Left child (\\( y \\le 2 \\))**  \n",
        "- Points = 5  \n",
        "- (+, −) = (4, 1)\n",
        "\n",
        "$$\n",
        "H_L =\n",
        "-\\left(\\frac{4}{5}\\log_2\\frac{4}{5}\n",
        "+ \\frac{1}{5}\\log_2\\frac{1}{5}\\right)\n",
        "\\approx 0.72\n",
        "$$\n",
        "\n",
        "**Right child (\\( y > 2 \\))**  \n",
        "- Points = 5  \n",
        "- (+, −) = (1, 4)\n",
        "\n",
        "$$\n",
        "H_R \\approx 0.72\n",
        "$$\n",
        "\n",
        "**Weighted entropy**\n",
        "\n",
        "$$\n",
        "H = \\frac{5}{10}(0.72) + \\frac{5}{10}(0.72) = 0.72\n",
        "$$\n",
        "\n",
        "**Entropy reduction**\n",
        "\n",
        "$$\n",
        "\\Delta H = 1.00 - 0.72 = \\mathbf{0.28}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Summary Table\n",
        "\n",
        "| Split        | Weighted Entropy | Entropy Reduction |\n",
        "|-------------|------------------|------------------|\n",
        "| \\( x \\le -2 \\) | 0.89 | 0.11 |\n",
        "| \\( x \\le 2 \\)  | 1.00 | 0.00 |\n",
        "| \\( y \\le 2 \\)  | 0.72 | **0.28** |\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Best split\n",
        "\n",
        "The split **\\( y \\le 2 \\)** should be chosen because it results in the **maximum reduction in entropy (0.28)**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpw9mvJtQ8Ox"
      },
      "source": [
        "### 3.2 Coding Question (Decision Tree using Iris Dataset)\n",
        "\n",
        "Write a Python program to **train and visualize a Decision Tree classifier** using the **Iris dataset**.\n",
        "\n",
        "Your code should:\n",
        "- Load the Iris dataset from `sklearn.datasets`\n",
        "- Split the data into **70% training** and **30% testing** sets\n",
        "- Train a Decision Tree classifier\n",
        "- Plot the learned decision tree with appropriate **feature names** and **class labels**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion=\"entropy\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 10))\n",
        "plot_tree(\n",
        "    clf,\n",
        "    feature_names=iris.feature_names,\n",
        "    class_names=iris.target_names,\n",
        "    filled=True,\n",
        "    rounded=True\n",
        ")\n",
        "plt.title(\"Decision Tree Classifier trained on Iris Dataset\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxR0Q0j2RC1Y"
      },
      "source": [
        "# Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmlSugnUKMNX"
      },
      "source": [
        "## Support Vector Machines (SVM)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1v88D6hCRLcJ"
      },
      "source": [
        "### 4.1 Theoretical\n",
        "\n",
        "a) Is a **Support Vector Machine (SVM)** a supervised or unsupervised learning algorithm?  \n",
        "Give a brief explanation.\n",
        "\n",
        "b) What is a **margin** in SVM?  \n",
        "Why does SVM aim to maximize the margin?\n",
        "\n",
        "c) What are **support vectors**?  \n",
        "Why are they important in defining the decision boundary?\n",
        "\n",
        "d) What is the purpose of a **kernel function** in SVM?  \n",
        "Name any two commonly used kernel functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (a) Is a Support Vector Machine supervised or unsupervised?\n",
        "\n",
        "A **Support Vector Machine (SVM)** is a **supervised learning algorithm** because it is trained on labeled data and learns a decision boundary (or hyperplane) that separates different classes or fits a regression target.\n",
        "\n",
        "---\n",
        "\n",
        "### (b) What is a margin in SVM? Why does SVM aim to maximize it?\n",
        "\n",
        "The **margin** is the distance between the decision boundary (hyperplane) and the closest data points from each class.\n",
        "SVM aims to **maximize the margin** because a larger margin leads to better generalization and makes the classifier more robust to noise and unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### (c) What are support vectors? Why are they important?\n",
        "\n",
        "**Support vectors** are the data points that lie closest to the decision boundary.\n",
        "They are important because they **define the position and orientation of the hyperplane**—removing other points does not change the decision boundary as long as the support vectors remain unchanged.\n",
        "\n",
        "---\n",
        "\n",
        "### (d) What is the purpose of a kernel function in SVM? Name two kernels.\n",
        "\n",
        "A **kernel function** allows SVM to handle **non-linearly separable data** by implicitly mapping the data into a higher-dimensional feature space where a linear separation is possible.\n",
        "\n",
        "Two commonly used kernel functions are:\n",
        "\n",
        "* **Linear kernel**\n",
        "* **Radial Basis Function (RBF) kernel**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryZVthcqROUl"
      },
      "source": [
        "### 4.2 Conceptual\n",
        "\n",
        "a) In a linearly separable dataset, how does SVM choose the **optimal separating hyperplane**?\n",
        "\n",
        "b) What happens when the data is **not linearly separable**?  \n",
        "Briefly explain how SVM handles this situation.\n",
        "\n",
        "c) What is the role of the **regularization parameter `C`** in SVM?  \n",
        "What happens when `C` is:\n",
        "- Very large  \n",
        "- Very small  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (a) How does SVM choose the optimal separating hyperplane in a linearly separable dataset?\n",
        "\n",
        "In a linearly separable dataset, SVM chooses the **hyperplane that maximizes the margin**, i.e., the distance between the hyperplane and the closest data points from each class. This hyperplane is considered optimal because it provides the best separation and improves generalization to unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "### (b) What happens when the data is not linearly separable?\n",
        "\n",
        "When the data is **not linearly separable**, SVM uses a **soft-margin approach** and/or **kernel functions**.\n",
        "The soft margin allows some data points to be misclassified by introducing slack variables, while kernel functions map the data into a higher-dimensional space where a linear separation becomes possible.\n",
        "\n",
        "---\n",
        "\n",
        "### (c) Role of the regularization parameter `C` in SVM\n",
        "\n",
        "The parameter **`C`** controls the trade-off between **maximizing the margin** and **minimizing classification errors**.\n",
        "\n",
        "* **Very large `C`**:\n",
        "  The model strongly penalizes misclassification, leading to a smaller margin and a more complex decision boundary that may overfit the training data.\n",
        "\n",
        "* **Very small `C`**:\n",
        "  The model allows more misclassifications, resulting in a larger margin and a simpler decision boundary that may underfit the data.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
